# Agents Learn Their Runtime

Official repository for the paper **"Agents Learn Their Runtime: Interpreter Persistence as Training-Time Semantics"**. This study isolates the effect of persistent executable state during training to determine if it functions as a learned inductive bias or an inference-time scaffold.

## Installation

Install the environment and dependencies
```bash
uv sync
uv pip install -e .
```

Install additional training dependencies if reproducing LoRA fine-tuning
```bash
uv sync --extra train
```

To run teacher trace generation, configure your LLM API key in a .env file:

```bash
GEMINI_API_KEY=...
```

## Task Instance Generation 
The paper uses Opaque Knapsack, a partially observable optimization task that requires iterative state revision. The tasks are generated via a procedural pipeline. To generate the task instances:

```bash
# Generate 1,000 'Easy' tasks for training
python cli.py --config task_configs/easy.json --out output/tasks/easy

# Generate 100 'Medium' tasks for OOD evaluation
python cli.py --config task_configs/medium.json --out output/tasks/medium
```

### Task JSON format (overview)

Each task is a single JSON object with:

- `task_id`, `family`, `seed`
- `tools`: declared tool contract for the episode
- `public`: visible instance data
- `private`: hidden environment state (for replay/verifier)
- `reference`: ground truth answer + helpful metadata
- `nl`: optional natural-language wrapper generated by an LLM


## Teacher Trace Generation
For our experiment, we train a base model on teacher traces generated with a CodeAct agent prompted to solve a set of task instances.

To generate traces using the persistent state regime:
```bash
uv run pythonformer/benchmark/benchmark.py --config generate_traces.persistent.yaml --tasks_root data/tasks_easy_1000 --max-examples 1000
```

To generate traces using the stateless regime:
```bash
uv run pythonformer/benchmark/benchmark.py --config generate_traces.stateless.yaml --tasks_root data/tasks_easy_1000 --max-examples 1000
```

## Trace Statistics Extraction
The following script parses execution logs from two agent execution output directories to generate comparative performance and efficiency metrics. It prints a side-by-side table comparing the two provided directories to highlight differences in agent regimes. The output table contains the following info for each run:

| Metric | Description |
| :--- | :--- |
| **Teacher Success Rate (%)** | The percentage of episodes where the agent successfully solved the task. |
| **Avg Teacher Optimality (0-1)** | The mean score (from 0 to 1) representing how close the agent's solution was to the theoretical optimum. |
| **Avg Capacity Utilization (%)** | The average percentage of total knapsack capacity filled by the agent's selections. |
| **Avg Items Inspected** | The average number of unique items the agent queried for information during the task. |

### Execution Efficiency
| Metric | Description |
| :--- | :--- |
| **Avg Steps / Episode** | The average number of high-level reasoning steps taken per task attempt. |
| **Avg Tool Calls / Episode** | The average count of specific API/tool function calls (e.g., `inspect`, `take_item`) per episode. |
| **Avg Total Tokens / Episode** | The mean number of total LLM tokens consumed per episode. |

To execute the statistics extraction script, run:
```bash
uv run python scripts/get_trace_stats.py <dir 1> <dir 2>
```

## Model Training & Inference
The following steps reproduce the transition from base models to the fine-tuned variants reported in the paper. All training uses 4-bit QLoRA with a 16k token context window, while inference scales to a 40k context window in native bf16. We used a single A100-80GB GPU for training.

### Model Training
We use Axolotl to fine-tune Qwen3-8B. The training scripts auto-convert the .trace.json files generated in the previous step into ShareGPT-format JSONL.
+2

```bash
# CodeAct Persistent State LoRA
make train-persistent

# CodeAct Reset State LoRA 
make train-forget

# ReAct LoRA (Baseline comparison)
make train-react
```

### Inference Server Setup
Evaluations require a running vLLM server. 

```bash
# Serve the base model (bf16, 40k ctx)
make serve

# Serve a specific LoRA adapter (e.g., Persistent)
make serve-persistent-lora
```

### Running Benchmarks
With the vLLM server active, run the benchmarking suite to evaluate performance against the Opaque Knapsack task instances.

```bash
# Evaluate Persistent LoRA on Easy tasks
make bench-easy-lora

# Evaluate Persistent LoRA on Medium (OOD) tasks
make bench-med-lora

# Perform Cross-Evaluation (Persistent LoRA in a Reset runtime)
make bench-cross-plora-reset-med
```

## Comparative Analysis and Statistical Significance
To reproduce the summary report comparing a pair of runs, use the following script. This tool pairs results by specific task IDs and performs an analysis of the performance delta between any two execution regimes or model variants.

### Generating Comparative Reports
The script processes two benchmark result tarballs to generate a side-by-side report:

```bash
# Compare Persistent LoRA vs. Reset LoRA on Medium tasks
uv run python scripts/compare_benchmarks.py results/med_persistent_lora.tar.gz results/med_reset_lora.tar.gz
```

### Generating the Main Results Table
This is done with the following script:

```bash
scripts/main_table.py table3_spec.json > table3.tex
```

---

## Spec File Format

Create a JSON spec file that tells the script where each cell's summary JSON lives.

### Top-level fields

| Field | Type | Default | Description |
|-------|------|---------|-------------|
| `caption` | string | (built-in default) | LaTeX table caption |
| `label` | string | `tab:main_results` | LaTeX `\label` key |
| `score_scale` | `"auto"` \| `"fraction"` \| `"percent"` | `"auto"` | How to interpret raw scores. `"auto"` detects fraction vs. percent by checking if the value is ≤ 1.5. |
| `solved_format` | `"count"` \| `"percent"` | `"count"` | Whether to show exact solves as a raw count or as a percentage of tasks executed. |

### Per-cell fields

| Field | Required | Description |
|-------|----------|-------------|
| `difficulty` | ✅ | `"Easy"` or `"Medium"` |
| `train` | ✅ | `"Persistent"` or `"Reset"` |
| `runtime` | ✅ | `"Persistent"` or `"Reset"` |
| `path` | ✅ | Path to the `summary.json` for this condition |
| `metrics_path` | ✗ | Path to a directory of `.trace.json` / `.json` result pairs. If provided, the script recomputes 95% bootstrap confidence intervals over per-episode scores (5,000 resamples). If omitted, no CI is shown. |
| `score_override` | ✗ | Float. If set, uses this value as the score instead of reading from `summary.json`. Useful when scores were computed separately. |
